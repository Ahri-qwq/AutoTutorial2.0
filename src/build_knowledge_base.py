import os
import glob
import uuid
import time
from typing import List

import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import dashscope
from dashscope import TextEmbedding
import yaml

try:
    from docx import Document
except ImportError:
    pass

# ================= è·¯å¾„é…ç½® =================
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(CURRENT_DIR)
CONFIG_PATH = os.path.join(ROOT_DIR, "config.yaml")
SOURCE_DIR = os.path.join(ROOT_DIR, "data", "knowledge_source")
DB_PATH = os.path.join(ROOT_DIR, "data", "chroma_db")
COLLECTION_NAME = "abacus_knowledge"

# ================= è¯»å– API Key =================
api_key = ""
try:
    if os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
            if config.get("llm"):
                api_key = config["llm"].get("api_key", "")
            else:
                api_key = config.get("api_key", "")
except Exception:
    pass

if not api_key:
    api_key = os.getenv("DASHSCOPE_API_KEY", "")

if api_key:
    dashscope.api_key = api_key
else:
    print("âŒ è­¦å‘Š: æœªæ‰¾åˆ° API Keyï¼Œåç»­è°ƒç”¨å°†å¤±è´¥ã€‚")

# ================= æ ¸å¿ƒç»„ä»¶ =================

class QwenEmbeddingFunction(EmbeddingFunction):
    """
    è‡ªå®šä¹‰çš„ ChromaDB åµŒå…¥å‡½æ•°ï¼Œä½¿ç”¨é˜¿é‡Œäº‘ Qwen Embedding API
    """
    def __call__(self, input: Documents) -> Embeddings:
        # ã€å…³é”®ä¿®æ”¹ã€‘é˜¿é‡Œäº‘é™åˆ¶ batch_size <= 10ï¼Œæˆ‘ä»¬è®¾ä¸º 5 ä»¥é˜²ä¸‡ä¸€
        batch_size = 5
        all_embeddings = []
        
        for i in range(0, len(input), batch_size):
            batch = input[i : i + batch_size]
            try:
                resp = TextEmbedding.call(
                    model=TextEmbedding.Models.text_embedding_v3,
                    input=batch
                )
                if resp.status_code == 200:
                    embeddings = [item['embedding'] for item in resp.output['embeddings']]
                    all_embeddings.extend(embeddings)
                else:
                    print(f"âš ï¸ API Error: {resp.message}")
                    all_embeddings.extend([[0.0]*1024 for _ in range(len(batch))])
            except Exception as e:
                print(f"âš ï¸ Network Exception: {e}")
                all_embeddings.extend([[0.0]*1024 for _ in range(len(batch))])
            
            # ç¨å¾®é™æµ
            time.sleep(0.2)
            
        return all_embeddings

def read_file(filepath: str) -> str:
    ext = os.path.splitext(filepath)[1].lower()
    content = ""
    try:
        if ext in [".md", ".txt"]:
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()
        elif ext == ".docx":
            if 'Document' in globals():
                doc = Document(filepath)
                content = "\n".join([p.text for p in doc.paragraphs])
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å– {filepath}: {e}")
    return content

def split_text(text: str, chunk_size=800, overlap=100) -> List[str]:
    if not text: return []
    chunks = []
    start = 0
    text_len = len(text)
    if text_len <= chunk_size: return [text]
    
    while start < text_len:
        end = min(start + chunk_size, text_len)
        chunk = text[start:end]
        chunks.append(chunk)
        if end == text_len: break
        start += (chunk_size - overlap)
    return chunks

def build_db():
    print(f"==========================================")
    print(f" ğŸš€ AutoTutorial çŸ¥è¯†åº“æ„å»º (Aliyunç‰ˆ) ")
    print(f"==========================================")
    print(f"ğŸ’¾ æ•°æ®åº“è·¯å¾„: {DB_PATH}")
    
    # 1. åˆå§‹åŒ–
    client = chromadb.PersistentClient(path=DB_PATH)
    
    # 2. æ¸…ç†æ—§é›†åˆ
    try:
        client.delete_collection(COLLECTION_NAME)
        print(f"ğŸ§¹ å·²æ¸…ç†æ—§é›†åˆ")
    except Exception:
        pass

    # 3. åˆ›å»ºæ–°é›†åˆ
    print("ğŸ”Œ è¿æ¥é˜¿é‡Œäº‘ Embedding API...")
    collection = client.create_collection(
        name=COLLECTION_NAME,
        embedding_function=QwenEmbeddingFunction()
    )

    # 4. æ‰«ææ–‡ä»¶
    if not os.path.exists(SOURCE_DIR):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {SOURCE_DIR}")
        return

    files = glob.glob(os.path.join(SOURCE_DIR, "**/*.md"), recursive=True) + \
            glob.glob(os.path.join(SOURCE_DIR, "**/*.docx"), recursive=True) + \
            glob.glob(os.path.join(SOURCE_DIR, "**/*.txt"), recursive=True)
    
    print(f"ğŸ“¦ æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
    total_chunks = 0
    
    for idx, filepath in enumerate(files):
        fname = os.path.basename(filepath)
        print(f"[{idx+1}/{len(files)}] å¤„ç†: {fname}...", end="", flush=True)
        
        content = read_file(filepath)
        if not content.strip():
            print(" [ç©º]")
            continue
            
        chunks = split_text(content)
        if not chunks: 
            print(" [æ— å†…å®¹]")
            continue

        ids = [f"{fname}_{i}_{str(uuid.uuid4())[:8]}" for i in range(len(chunks))]
        metadatas = [{"source": fname, "chunk_index": i} for i in range(len(chunks))]
        
        # å†™å…¥
        try:
            collection.add(documents=chunks, metadatas=metadatas, ids=ids)
            print(f" âœ… {len(chunks)} ç‰‡æ®µ")
            total_chunks += len(chunks)
        except Exception as e:
            print(f" âŒ å†™å…¥å¤±è´¥: {e}")

    print(f"\nğŸ‰ æ„å»ºå®Œæˆï¼æ€»ç‰‡æ®µ: {total_chunks}")
    print(f"ğŸ’¾ æ•°æ®åº“å·²ä¿å­˜è‡³: {DB_PATH}")

if __name__ == "__main__":
    build_db()
