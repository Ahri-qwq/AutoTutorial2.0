import os
import glob
import uuid
import time
import shutil
from typing import List

try:
    from pypdf import PdfReader
except ImportError:
    PdfReader = None


import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings

import dashscope
from dashscope import TextEmbedding
import yaml

try:
    from docx import Document
except ImportError:
    Document = None

# ================= è·¯å¾„é…ç½®ï¼ˆä¸ build_knowledge_base.py ä¿æŒä¸€è‡´ï¼‰ =================

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(CURRENT_DIR)

CONFIG_PATH = os.path.join(ROOT_DIR, "config.yaml")
SOURCE_DIR = os.path.join(ROOT_DIR, "data", "knowledge_add")   # â˜… åªè¯»å¢é‡ç›®å½•
DB_PATH = os.path.join(ROOT_DIR, "data", "chroma_db")
COLLECTION_NAME = "abacus_knowledge"

# ================= è¯»å– API Keyï¼ˆå®Œå…¨ç…§æŠ„ build_knowledge_base.pyï¼‰ =================

api_key = ""
try:
    if os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        if config.get("llm"):
            api_key = config["llm"].get("api_key", "")
        else:
            api_key = config.get("api_key", "")
except Exception:
    pass

if not api_key:
    api_key = os.getenv("DASHSCOPE_API_KEY", "")

if api_key:
    dashscope.api_key = api_key
else:
    print("âŒ è­¦å‘Š: æœªæ‰¾åˆ° API Keyï¼Œåç»­è°ƒç”¨å°†å¤±è´¥ã€‚")

# ================= æ ¸å¿ƒç»„ä»¶ï¼ˆä¸ build_knowledge_base.py ä¿æŒä¸€è‡´ï¼‰ =================

class QwenEmbeddingFunction(EmbeddingFunction):
    """
    è‡ªå®šä¹‰çš„ ChromaDB åµŒå…¥å‡½æ•°ï¼Œä½¿ç”¨é˜¿é‡Œäº‘ Qwen Embedding API
    """

    def __call__(self, input: Documents) -> Embeddings:
        # é˜¿é‡Œäº‘é™åˆ¶ batch_size <= 10ï¼Œè¿™é‡Œä¿æŒå’Œå…¨é‡è„šæœ¬ä¸€è‡´
        batch_size = 5
        all_embeddings = []
        for i in range(0, len(input), batch_size):
            batch = input[i : i + batch_size]
            try:
                resp = TextEmbedding.call(
                    model=TextEmbedding.Models.text_embedding_v3,
                    input=batch
                )
                if resp.status_code == 200:
                    embeddings = [item["embedding"] for item in resp.output["embeddings"]]
                    all_embeddings.extend(embeddings)
                else:
                    print(f"âš ï¸ API Error: {resp.message}")
                    all_embeddings.extend([[0.0] * 1024 for _ in range(len(batch))])
            except Exception as e:
                print(f"âš ï¸ Network Exception: {e}")
                all_embeddings.extend([[0.0] * 1024 for _ in range(len(batch))])
            # ç¨å¾®é™æµ
            time.sleep(0.2)
        return all_embeddings

# ================= å·¥å…·å‡½æ•°ï¼ˆä¸ build_knowledge_base.py ä¿æŒä¸€è‡´ï¼‰ =================

def read_file(filepath: str) -> str:
    ext = os.path.splitext(filepath)[1].lower()
    content = ""
    try:
        if ext in [".md", ".txt"]:
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()
        elif ext == ".docx":
            if "Document" in globals() and Document is not None:
                doc = Document(filepath)
                content = "\n".join([p.text for p in doc.paragraphs])
        elif ext == ".pdf":
            if PdfReader is None:
                print(f"âš ï¸ è·³è¿‡ PDFï¼ˆæœªå®‰è£… pypdfï¼‰: {filepath}")
                return ""
            reader = PdfReader(filepath)
            pages = []
            for page in reader.pages:
                t = page.extract_text() or ""
                pages.append(t)
            content = "\n".join(pages)
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å– {filepath}: {e}")
    return content


def split_text(text: str, chunk_size=800, overlap=100) -> List[str]:
    if not text:
        return []
    chunks: List[str] = []
    start = 0
    text_len = len(text)
    if text_len <= chunk_size:
        return [text]
    while start < text_len:
        end = min(start + chunk_size, text_len)
        chunk = text[start:end]
        chunks.append(chunk)
        if end == text_len:
            break
        start += (chunk_size - overlap)
    return chunks

# ================= å¢é‡è¿½åŠ é€»è¾‘ =================

def append_db():
    print("==========================================")
    print(" ğŸ§© AutoTutorial çŸ¥è¯†åº“å¢é‡è¿½åŠ  (Aliyunç‰ˆ) ")
    print("==========================================")
    print(f"ğŸ’¾ æ•°æ®åº“è·¯å¾„: {DB_PATH}")
    print(f"ğŸ“‚ å¢é‡ç›®å½•: {SOURCE_DIR}")

    client = chromadb.PersistentClient(path=DB_PATH)

    collection = client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=QwenEmbeddingFunction()
    )

    if not os.path.exists(SOURCE_DIR):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {SOURCE_DIR}")
        return

    files = (
        glob.glob(os.path.join(SOURCE_DIR, "**/*.md"), recursive=True)
        + glob.glob(os.path.join(SOURCE_DIR, "**/*.docx"), recursive=True)
        + glob.glob(os.path.join(SOURCE_DIR, "**/*.txt"), recursive=True)
        + glob.glob(os.path.join(SOURCE_DIR, "**/*.pdf"), recursive=True)
    )

    print(f"ğŸ“¦ æ‰¾åˆ° {len(files)} ä¸ªå¢é‡æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
    if not files:
        return

    total_chunks = 0
    # è®°å½•è‡³å°‘æœ‰ä¸€æ®µæˆåŠŸå†™å…¥çš„æ–‡ä»¶è·¯å¾„
    ingested_files = []

    for idx, filepath in enumerate(files):
        fname = os.path.basename(filepath)
        print(f"[{idx+1}/{len(files)}] å¤„ç†: {fname}...", end="", flush=True)

        content = read_file(filepath)
        if not content.strip():
            print(" [ç©º]")
            continue

        chunks = split_text(content)
        if not chunks:
            print(" [æ— å†…å®¹]")
            continue

        ids = [f"{fname}_{i}_{str(uuid.uuid4())[:8]}" for i in range(len(chunks))]
        metadatas = [
            {"source": fname, "chunk_index": i, "ingest_type": "append"}
            for i in range(len(chunks))
        ]

        try:
            collection.add(documents=chunks, metadatas=metadatas, ids=ids)
            print(f" âœ… {len(chunks)} ç‰‡æ®µ")
            total_chunks += len(chunks)
            ingested_files.append(filepath)  # åªè¦è¿™ä¸€æ–‡ä»¶æœ‰ä¸€æ¬¡æˆåŠŸå†™å…¥ï¼Œå°±æ ‡è®°å½’æ¡£
        except Exception as e:
            print(f" âŒ å†™å…¥å¤±è´¥: {e}")

    print(f"\nğŸ‰ å¢é‡è¿½åŠ å®Œæˆï¼æœ¬æ¬¡æ–°å¢ç‰‡æ®µ: {total_chunks}")
    print(f"ğŸ’¾ æ•°æ®åº“è·¯å¾„: {DB_PATH}")

    # ========== è‡ªåŠ¨å½’æ¡£åˆ° knowledge_source ==========
    target_root = os.path.join(ROOT_DIR, "data", "knowledge_source")
    os.makedirs(target_root, exist_ok=True)

    moved = 0
    for src_path in ingested_files:
        # ä¿ç•™ç›¸å¯¹ç›®å½•ç»“æ„ï¼šknowledge_add/... -> knowledge_source/...
        rel_path = os.path.relpath(src_path, SOURCE_DIR)
        dst_path = os.path.join(target_root, rel_path)
        dst_dir = os.path.dirname(dst_path)
        os.makedirs(dst_dir, exist_ok=True)
        try:
            shutil.move(src_path, dst_path)
            moved += 1
        except Exception as e:
            print(f"âš ï¸ å½’æ¡£å¤±è´¥ {src_path} -> {dst_path}: {e}")

    print(f"ğŸ“‚ å·²è‡ªåŠ¨å½’æ¡£ {moved} ä¸ªæ–‡ä»¶åˆ° knowledge_sourceã€‚")


if __name__ == "__main__":
    append_db()
